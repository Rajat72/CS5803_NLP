{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "## Roll Number : CS25MTECH02007\n",
    "## Name : Rajat Maheshwari\n",
    "<hr>\n",
    "\n",
    "# Text Cleaning, and Model Building For Given DataSets\n",
    "\n",
    "## üìù Assignment Overview\n",
    "In this assignment, I will:\n",
    "1. **Implement** 4 different NLP classification models\n",
    "2. **Clean and harmonize**  Given Data for model building\n",
    "3. Perform **text-based Classification** on the cleaned data to extract insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from -r requirements.txt (line 2)) (1.6.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from -r requirements.txt (line 3)) (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from -r requirements.txt (line 4)) (2.2.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from -r requirements.txt (line 5)) (3.9.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from -r requirements.txt (line 6)) (3.8.4)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from -r requirements.txt (line 7)) (6.29.5)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from -r requirements.txt (line 8)) (8.1.5)\n",
      "Requirement already satisfied: keras in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from -r requirements.txt (line 9)) (3.8.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from -r requirements.txt (line 10)) (2.18.0)\n",
      "Requirement already satisfied: torch in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from -r requirements.txt (line 11)) (2.6.0)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from -r requirements.txt (line 12)) (3.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (2024.12.14)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 2)) (3.5.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 3)) (2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2025.1)\n",
      "Requirement already satisfied: click in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from nltk->-r requirements.txt (line 5)) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from nltk->-r requirements.txt (line 5)) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from nltk->-r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (0.15.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (2.10.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from spacy->-r requirements.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 7)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 7)) (1.8.12)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 7)) (8.31.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 7)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 7)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 7)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 7)) (1.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 7)) (6.1.1)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 7)) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 7)) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 7)) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipywidgets->-r requirements.txt (line 8)) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipywidgets->-r requirements.txt (line 8)) (3.0.13)\n",
      "Requirement already satisfied: absl-py in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from keras->-r requirements.txt (line 9)) (2.1.0)\n",
      "Requirement already satisfied: rich in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from keras->-r requirements.txt (line 9)) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from keras->-r requirements.txt (line 9)) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from keras->-r requirements.txt (line 9)) (3.13.0)\n",
      "Requirement already satisfied: optree in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from keras->-r requirements.txt (line 9)) (0.14.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from keras->-r requirements.txt (line 9)) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorflow->-r requirements.txt (line 10)) (2.18.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (5.29.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (2.18.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from torch->-r requirements.txt (line 11)) (3.17.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from torch->-r requirements.txt (line 11)) (3.4.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from torch->-r requirements.txt (line 11)) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from torch->-r requirements.txt (line 11)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from sympy==1.13.1->torch->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from openpyxl->-r requirements.txt (line 12)) (2.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 7)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 7)) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 7)) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 7)) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 7)) (0.6.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 7)) (4.3.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 7)) (308)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 6)) (2.27.2)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 6)) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 6)) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from rich->keras->-r requirements.txt (line 9)) (3.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 6)) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 6)) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from jinja2->spacy->-r requirements.txt (line 6)) (3.0.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (0.45.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r requirements.txt (line 7)) (0.8.4)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 6)) (1.2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->-r requirements.txt (line 9)) (0.1.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r requirements.txt (line 7)) (0.2.13)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->-r requirements.txt (line 10)) (3.1.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\rajat\\onedrive\\documents\\iit h\\sem-1\\nlp\\cs5803_nlp\\venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 7)) (0.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import urllib\n",
    "import unicodedata\n",
    "import urllib.parse\n",
    "import logging\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\rajat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rajat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rajat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rajat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rajat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\rajat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaner Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        # Initialize our regex detectives - these patterns help sniff out specfic data types\n",
    "        self.json_pattern = re.compile(r'\\{[^}]+\\}', re.DOTALL)  # Catches JSON blobs like {key:val}\n",
    "        self.math_pattern = re.compile(r'\\$.*?\\$', re.DOTALL)     # Finds mathy stuff between $$\n",
    "        self.control_chars = re.compile(r\"[\\x00-\\x09\\x0B\\x0C\\x0E-\\x1F\\x7F-\\x9F]\")  # Weird control chars\n",
    "        self.extra_newlines = re.compile(r'\\n{9,}')  \n",
    "        self.non_word_chars = re.compile(r\"[^\\w\\s]\")  # Non-word chars except spaces\n",
    "        self.multi_space = re.compile(r\" +\")  # Squash multiple spacces\n",
    "\n",
    "        # Set up NLP tools - lemmatizer for word roots, stopwords for common junk\n",
    "        self.lemmatizer = WordNetLemmatizer()  # Makes words their base form (better -> good)\n",
    "        self.stop_words = set(stopwords.words('english'))  # Common words like 'the', 'and'\n",
    "\n",
    "    def _get_wordnet_pos(self, treebank_tag):\n",
    "        # Helper to convert POS tags to wordnet format. Treebank tags are confusing TBH\n",
    "        return {\n",
    "            'J': wordnet.ADJ,  # Adjectives\n",
    "            'V': wordnet.VERB,  # Verbs\n",
    "            'R': wordnet.ADV  # Adverbs\n",
    "        }.get(treebank_tag[0], wordnet.NOUN)  # Default to noun if no match\n",
    "\n",
    "    def lemmatize_text(self, text, use_pos=True):\n",
    "        \"\"\"Turns words to their dictionary form. POS tagging helps accuracy but costs speed.\"\"\"\n",
    "        try:\n",
    "            tokens = word_tokenize(text)  # Split text into words\n",
    "            \n",
    "            if use_pos:  \n",
    "                pos_tags = nltk.pos_tag(tokens)  # Get grammar tags\n",
    "                return ' '.join([\n",
    "                    self.lemmatizer.lemmatize(word, self._get_wordnet_pos(tag))\n",
    "                    for word, tag in pos_tags  # Process each word with its tag\n",
    "                ])\n",
    "            else:\n",
    "                return ' '.join([self.lemmatizer.lemmatize(word) for word in tokens])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Lemmatization oopsie: {str(e)}\")  # Log but don't crash\n",
    "            return text  # Return original if things go south\n",
    "\n",
    "    def remove_numbers(self, text):\n",
    "        \"\"\"Kicks out lonely numbers but keeps words with numbers (like B2B)\"\"\"\n",
    "        return re.sub(r'\\b\\d+\\b', '', text)  # \\b means word boundary\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Main scrubber - handles encoding weirdness and special characters\"\"\"\n",
    "        try:\n",
    "            text = self.control_chars.sub(\" \", text)  # Remove control chars\n",
    "            text = unicodedata.normalize(\"NFKD\", text)  # Standardize fancy unicode\n",
    "            text = text.replace(\"\\u2022\", \"\\n- \").replace(\"\\xa0\", \" \")  # Bullets to dashes\n",
    "            text = self.non_word_chars.sub(\" \", text)  # Replace punctuation\n",
    "            text = text.encode(\"ascii\", \"ignore\").decode(\"utf-8\")  # Force ASCII\n",
    "            text = self.extra_newlines.sub(\"\\n\\n\", text)  # Limit excessive newlines\n",
    "            return self.multi_space.sub(\" \", text).strip()  # Clean up spaces\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Cleaner tripped up: {str(e)}\")  # Non-fatal error\n",
    "            return text\n",
    "        \n",
    "    def clean_ratings(self, text):\n",
    "        \"\"\"Swaps number+letter counts like 12M ‚Üí COUNT. Handy for review data\"\"\"\n",
    "        return re.sub(r'\\b\\d+[MK]\\b', 'COUNT', text, flags=re.IGNORECASE) \n",
    "    \n",
    "    def clean_durations(self, text):\n",
    "        \"\"\"Converts time formats like 2h 30m ‚Üí Duration\"\"\"\n",
    "        return re.sub(r'\\b\\d+h\\s\\d+m\\b', 'Duration', text, flags=re.IGNORECASE)\n",
    "\n",
    "    def full_clean(self, text, is_title=False, remove_stopwords=True, \n",
    "                    lemmatize=True, remove_numbers=False,remove_duration=False,remove_rating=False):\n",
    "            \"\"\"\n",
    "            Master cleaning pipeline with toggleable features. Goes from raw text ‚Üí squeaky clean.\n",
    "            \n",
    "            Params:\n",
    "            - is_title: Special handling for titles/headers\n",
    "            - remove_stopwords: Cut common words (the, a, etc)\n",
    "            - lemmatize: Reduce words to base form\n",
    "            - remove_numbers: Strip standalone numbers\n",
    "            \"\"\"\n",
    "            try:\n",
    "                # Phase 1: Structure cleanup\n",
    "                text = self.json_pattern.sub('', text)  # remove JSON objects\n",
    "                text = self.math_pattern.sub('Mathemtical Expression', text)  \n",
    "                \n",
    "                # Phase 2: Text normalization\n",
    "                text = self.clean_title(text) if is_title else self.clean_text(text)\n",
    "                \n",
    "                # Phase 3: Number cleanup \n",
    "                if remove_numbers:\n",
    "                    text = self.remove_numbers(text)  # Bye-bye lonely digits\n",
    "                \n",
    "                # Phase 4: Linguistic processing\n",
    "                if remove_stopwords:\n",
    "                    text = ' '.join([word for word in text.split() \n",
    "                                if word.lower() not in self.stop_words])  # Filter common words\n",
    "                \n",
    "                if lemmatize:  # Base word forms\n",
    "                    text = self.lemmatize_text(text)\n",
    "                \n",
    "                if remove_duration:  # Time formats\n",
    "                    text = self.clean_durations(text)\n",
    "                \n",
    "                if remove_rating:  # Like 1.2M ratings\n",
    "                    text = self.clean_ratings(text)\n",
    "                \n",
    "                return self.multi_space.sub(' ', text).strip()  \n",
    "            \n",
    "            except Exception as e:  # Oops, something broke\n",
    "                logging.error(f\"Full clean pipeline glitch: {str(e)}\") \n",
    "                return text  # Return whatever we have\n",
    "\n",
    "    def clean_title(self, text):\n",
    "            \"\"\"Special handling for titles - extra URL decoding and underscore fixes\"\"\"\n",
    "            cleaned = self.clean_text(text)  # Do normal cleaning first\n",
    "            cleaned = urllib.parse.unquote(cleaned)  # Convert %20 to spaces etc\n",
    "            cleaned = re.sub(r\"_+\", \" \", cleaned)  # Underscores ‚Üí spaces\n",
    "            return self.multi_space.sub(\" \", cleaned).strip()  # Final whitespace pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=pd.read_excel(\"data\\\\Dataset-1.xlsx\")\n",
    "data1_copy=data1\n",
    "data2=pd.read_excel(\"data\\\\Dataset-2.xlsx\")\n",
    "data2_copy=data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean DataSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaner Objects Intialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = TextCleaner()\n",
    "encoder = OneHotEncoder(sparse_output=False,dtype=np.int8,feature_name_combiner=lambda feature, category: str(category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m data1_copy\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m],inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m data1_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mABSTRACT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mABSTRACT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcleaner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_stopwords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemmatize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m data1_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTITLE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTITLE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: cleaner\u001b[38;5;241m.\u001b[39mfull_clean(x, remove_stopwords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, lemmatize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, is_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m data1_copy\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntermediate\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata1_pass1.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\rajat\\OneDrive\\Documents\\IIT H\\Sem-1\\NLP\\CS5803_NLP\\venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rajat\\OneDrive\\Documents\\IIT H\\Sem-1\\NLP\\CS5803_NLP\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rajat\\OneDrive\\Documents\\IIT H\\Sem-1\\NLP\\CS5803_NLP\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\rajat\\OneDrive\\Documents\\IIT H\\Sem-1\\NLP\\CS5803_NLP\\venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rajat\\OneDrive\\Documents\\IIT H\\Sem-1\\NLP\\CS5803_NLP\\venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[99], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m data1_copy\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m],inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m data1_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mABSTRACT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mABSTRACT\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mcleaner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_stopwords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemmatize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m data1_copy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTITLE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTITLE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: cleaner\u001b[38;5;241m.\u001b[39mfull_clean(x, remove_stopwords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, lemmatize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, is_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m data1_copy\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntermediate\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata1_pass1.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[95], line 90\u001b[0m, in \u001b[0;36mTextCleaner.full_clean\u001b[1;34m(self, text, is_title, remove_stopwords, lemmatize, remove_numbers, remove_duration, remove_rating)\u001b[0m\n\u001b[0;32m     86\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit() \n\u001b[0;32m     87\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_words])\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lemmatize:\n\u001b[1;32m---> 90\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_duration:\n\u001b[0;32m     93\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_durations(text)\n",
      "Cell \u001b[1;32mIn[95], line 26\u001b[0m, in \u001b[0;36mTextCleaner.lemmatize_text\u001b[1;34m(self, text, use_pos)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Perform lemmatization with POS tagging\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_pos:\n\u001b[0;32m     28\u001b[0m         pos_tags \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mpos_tag(tokens)\n",
      "File \u001b[1;32mc:\\Users\\rajat\\OneDrive\\Documents\\IIT H\\Sem-1\\NLP\\CS5803_NLP\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:144\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 144\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\rajat\\OneDrive\\Documents\\IIT H\\Sem-1\\NLP\\CS5803_NLP\\venv\\Lib\\site-packages\\nltk\\tokenize\\destructive.py:158\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    150\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn_str\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been deprecated and should no \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlonger be used.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    153\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m    154\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    155\u001b[0m     )\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTARTING_QUOTES:\n\u001b[1;32m--> 158\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPUNCTUATION:\n\u001b[0;32m    161\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# remove \"ID\" Column\n",
    "data1_copy.drop(columns=[\"ID\"],inplace=True)\n",
    "\n",
    "#Clean text Data on various parameters\n",
    "data1_copy['ABSTRACT'] = data1['ABSTRACT'].apply(lambda x: cleaner.full_clean(x, remove_stopwords=True, lemmatize=True))\n",
    "data1_copy['TITLE'] = data1['TITLE'].apply(lambda x: cleaner.full_clean(x, remove_stopwords=True, lemmatize=True, is_title=True))\n",
    "\n",
    "#save intermediate form\n",
    "data1_copy.to_excel(\"Intermediate\\\\data1_pass1.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"ID\" Column\n",
    "data2_copy.drop(columns=[\"ID\"],inplace=True)\n",
    "\n",
    "#Clean text Data on various parameters\n",
    "data2_copy['Content'] = data2['Content'].apply(\n",
    "lambda x: cleaner.full_clean(x, remove_stopwords=True, lemmatize=True, remove_numbers=True,remove_duration=True,remove_rating=True)\n",
    ")\n",
    "\n",
    "#Change Domain to One-hot Encoding Format\n",
    "encoded_array = encoder.fit_transform(data2_copy[[\"Domain\"]])\n",
    "new_columns = encoder.get_feature_names_out()\n",
    "data2_copy = pd.concat([data2_copy.drop(\"Domain\", axis=1),pd.DataFrame(encoded_array, columns=new_columns)], axis=1)\n",
    "\n",
    "\n",
    "#save intermediate form\n",
    "data2_copy.to_excel(\"Intermediate\\\\data2_pass1.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
